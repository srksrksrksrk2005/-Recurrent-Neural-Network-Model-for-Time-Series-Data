{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a48f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50b9200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question      answer\n",
       "0                   What is the capital of France?       Paris\n",
       "1                  What is the capital of Germany?      Berlin\n",
       "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
       "3  What is the largest planet in our solar system?     Jupiter\n",
       "4   What is the boiling point of water in Celsius?         100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('100_Unique_QA_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3789172",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['question']\n",
    "Y = df[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4f07a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     What is the capital of France?\n",
       "1                    What is the capital of Germany?\n",
       "2                 Who wrote 'To Kill a Mockingbird'?\n",
       "3    What is the largest planet in our solar system?\n",
       "4     What is the boiling point of water in Celsius?\n",
       "Name: question, dtype: str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b030a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Paris\n",
       "1        Berlin\n",
       "2    Harper-Lee\n",
       "3       Jupiter\n",
       "4           100\n",
       "Name: answer, dtype: str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed5676a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('?','')\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "053cb0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'the', 'capital', 'of', 'france']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b120898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"<UNK>\":0}\n",
    "def build_vocab(row):\n",
    "    token = tokenize(row[\"question\"]) + tokenize(row[\"answer\"])\n",
    "    for i in token :\n",
    "        if i not in vocab:\n",
    "            vocab[i] = len(vocab)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a91381d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [what, is, the, capital, of, france, paris]\n",
       "1         [what, is, the, capital, of, germany, berlin]\n",
       "2     [who, wrote, to, kill, a, mockingbird, harper-...\n",
       "3     [what, is, the, largest, planet, in, our, sola...\n",
       "4     [what, is, the, boiling, point, of, water, in,...\n",
       "                            ...                        \n",
       "85    [who, directed, the, movie, titanic, jamescame...\n",
       "86    [which, superhero, is, also, known, as, the, d...\n",
       "87       [what, is, the, capital, of, brazil, brasilia]\n",
       "88    [which, fruit, is, known, as, the, king, of, f...\n",
       "89    [which, country, is, known, for, the, eiffel, ...\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(build_vocab,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "340b6cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'capital': 4,\n",
       " 'of': 5,\n",
       " 'france': 6,\n",
       " 'paris': 7,\n",
       " 'germany': 8,\n",
       " 'berlin': 9,\n",
       " 'who': 10,\n",
       " 'wrote': 11,\n",
       " 'to': 12,\n",
       " 'kill': 13,\n",
       " 'a': 14,\n",
       " 'mockingbird': 15,\n",
       " 'harper-lee': 16,\n",
       " 'largest': 17,\n",
       " 'planet': 18,\n",
       " 'in': 19,\n",
       " 'our': 20,\n",
       " 'solar': 21,\n",
       " 'system': 22,\n",
       " 'jupiter': 23,\n",
       " 'boiling': 24,\n",
       " 'point': 25,\n",
       " 'water': 26,\n",
       " 'celsius': 27,\n",
       " '100': 28,\n",
       " 'painted': 29,\n",
       " 'mona': 30,\n",
       " 'lisa': 31,\n",
       " 'leonardo-da-vinci': 32,\n",
       " 'square': 33,\n",
       " 'root': 34,\n",
       " '64': 35,\n",
       " '8': 36,\n",
       " 'chemical': 37,\n",
       " 'symbol': 38,\n",
       " 'for': 39,\n",
       " 'gold': 40,\n",
       " 'au': 41,\n",
       " 'which': 42,\n",
       " 'year': 43,\n",
       " 'did': 44,\n",
       " 'world': 45,\n",
       " 'war': 46,\n",
       " 'ii': 47,\n",
       " 'end': 48,\n",
       " '1945': 49,\n",
       " 'longest': 50,\n",
       " 'river': 51,\n",
       " 'nile': 52,\n",
       " 'japan': 53,\n",
       " 'tokyo': 54,\n",
       " 'developed': 55,\n",
       " 'theory': 56,\n",
       " 'relativity': 57,\n",
       " 'albert-einstein': 58,\n",
       " 'freezing': 59,\n",
       " 'fahrenheit': 60,\n",
       " '32': 61,\n",
       " 'known': 62,\n",
       " 'as': 63,\n",
       " 'red': 64,\n",
       " 'mars': 65,\n",
       " 'author': 66,\n",
       " '1984': 67,\n",
       " 'george-orwell': 68,\n",
       " 'currency': 69,\n",
       " 'united': 70,\n",
       " 'kingdom': 71,\n",
       " 'pound': 72,\n",
       " 'india': 73,\n",
       " 'delhi': 74,\n",
       " 'discovered': 75,\n",
       " 'gravity': 76,\n",
       " 'newton': 77,\n",
       " 'how': 78,\n",
       " 'many': 79,\n",
       " 'continents': 80,\n",
       " 'are': 81,\n",
       " 'there': 82,\n",
       " 'on': 83,\n",
       " 'earth': 84,\n",
       " '7': 85,\n",
       " 'gas': 86,\n",
       " 'do': 87,\n",
       " 'plants': 88,\n",
       " 'use': 89,\n",
       " 'photosynthesis': 90,\n",
       " 'co2': 91,\n",
       " 'smallest': 92,\n",
       " 'prime': 93,\n",
       " 'number': 94,\n",
       " '2': 95,\n",
       " 'invented': 96,\n",
       " 'telephone': 97,\n",
       " 'alexander-graham-bell': 98,\n",
       " 'australia': 99,\n",
       " 'canberra': 100,\n",
       " 'ocean': 101,\n",
       " 'pacific-ocean': 102,\n",
       " 'speed': 103,\n",
       " 'light': 104,\n",
       " 'vacuum': 105,\n",
       " '299,792,458m/s': 106,\n",
       " 'language': 107,\n",
       " 'spoken': 108,\n",
       " 'brazil': 109,\n",
       " 'portuguese': 110,\n",
       " 'penicillin': 111,\n",
       " 'alexander-fleming': 112,\n",
       " 'canada': 113,\n",
       " 'ottawa': 114,\n",
       " 'mammal': 115,\n",
       " 'whale': 116,\n",
       " 'element': 117,\n",
       " 'has': 118,\n",
       " 'atomic': 119,\n",
       " '1': 120,\n",
       " 'hydrogen': 121,\n",
       " 'tallest': 122,\n",
       " 'mountain': 123,\n",
       " 'everest': 124,\n",
       " 'city': 125,\n",
       " 'big': 126,\n",
       " 'apple': 127,\n",
       " 'newyork': 128,\n",
       " 'planets': 129,\n",
       " 'starry': 130,\n",
       " 'night': 131,\n",
       " 'vangogh': 132,\n",
       " 'formula': 133,\n",
       " 'h2o': 134,\n",
       " 'italy': 135,\n",
       " 'rome': 136,\n",
       " 'country': 137,\n",
       " 'famous': 138,\n",
       " 'sushi': 139,\n",
       " 'was': 140,\n",
       " 'first': 141,\n",
       " 'person': 142,\n",
       " 'step': 143,\n",
       " 'moon': 144,\n",
       " 'armstrong': 145,\n",
       " 'main': 146,\n",
       " 'ingredient': 147,\n",
       " 'guacamole': 148,\n",
       " 'avocado': 149,\n",
       " 'sides': 150,\n",
       " 'does': 151,\n",
       " 'hexagon': 152,\n",
       " 'have': 153,\n",
       " '6': 154,\n",
       " 'china': 155,\n",
       " 'yuan': 156,\n",
       " 'pride': 157,\n",
       " 'and': 158,\n",
       " 'prejudice': 159,\n",
       " 'jane-austen': 160,\n",
       " 'iron': 161,\n",
       " 'fe': 162,\n",
       " 'hardest': 163,\n",
       " 'natural': 164,\n",
       " 'substance': 165,\n",
       " 'diamond': 166,\n",
       " 'continent': 167,\n",
       " 'by': 168,\n",
       " 'area': 169,\n",
       " 'asia': 170,\n",
       " 'president': 171,\n",
       " 'states': 172,\n",
       " 'george-washington': 173,\n",
       " 'bird': 174,\n",
       " 'its': 175,\n",
       " 'ability': 176,\n",
       " 'mimic': 177,\n",
       " 'sounds': 178,\n",
       " 'parrot': 179,\n",
       " 'longest-running': 180,\n",
       " 'animated': 181,\n",
       " 'tv': 182,\n",
       " 'show': 183,\n",
       " 'simpsons': 184,\n",
       " 'vaticancity': 185,\n",
       " 'most': 186,\n",
       " 'moons': 187,\n",
       " 'saturn': 188,\n",
       " 'romeo': 189,\n",
       " 'juliet': 190,\n",
       " 'shakespeare': 191,\n",
       " 'earths': 192,\n",
       " 'atmosphere': 193,\n",
       " 'nitrogen': 194,\n",
       " 'bones': 195,\n",
       " 'adult': 196,\n",
       " 'human': 197,\n",
       " 'body': 198,\n",
       " '206': 199,\n",
       " 'metal': 200,\n",
       " 'liquid': 201,\n",
       " 'at': 202,\n",
       " 'room': 203,\n",
       " 'temperature': 204,\n",
       " 'mercury': 205,\n",
       " 'russia': 206,\n",
       " 'moscow': 207,\n",
       " 'electricity': 208,\n",
       " 'benjamin-franklin': 209,\n",
       " 'second-largest': 210,\n",
       " 'land': 211,\n",
       " 'color': 212,\n",
       " 'ripe': 213,\n",
       " 'banana': 214,\n",
       " 'yellow': 215,\n",
       " 'month': 216,\n",
       " '28': 217,\n",
       " 'days': 218,\n",
       " 'common': 219,\n",
       " 'february': 220,\n",
       " 'study': 221,\n",
       " 'living': 222,\n",
       " 'organisms': 223,\n",
       " 'called': 224,\n",
       " 'biology': 225,\n",
       " 'home': 226,\n",
       " 'great': 227,\n",
       " 'wall': 228,\n",
       " 'bees': 229,\n",
       " 'collect': 230,\n",
       " 'from': 231,\n",
       " 'flowers': 232,\n",
       " 'nectar': 233,\n",
       " 'opposite': 234,\n",
       " 'day': 235,\n",
       " 'south': 236,\n",
       " 'korea': 237,\n",
       " 'seoul': 238,\n",
       " 'bulb': 239,\n",
       " 'edison': 240,\n",
       " 'humans': 241,\n",
       " 'breathe': 242,\n",
       " 'survival': 243,\n",
       " 'oxygen': 244,\n",
       " '144': 245,\n",
       " '12': 246,\n",
       " 'pyramids': 247,\n",
       " 'giza': 248,\n",
       " 'egypt': 249,\n",
       " 'sea': 250,\n",
       " 'creature': 251,\n",
       " 'eight': 252,\n",
       " 'arms': 253,\n",
       " 'octopus': 254,\n",
       " 'holiday': 255,\n",
       " 'celebrated': 256,\n",
       " 'december': 257,\n",
       " '25': 258,\n",
       " 'christmas': 259,\n",
       " 'yen': 260,\n",
       " 'legs': 261,\n",
       " 'spider': 262,\n",
       " 'sport': 263,\n",
       " 'uses': 264,\n",
       " 'net,': 265,\n",
       " 'ball,': 266,\n",
       " 'hoop': 267,\n",
       " 'basketball': 268,\n",
       " 'kangaroos': 269,\n",
       " 'female': 270,\n",
       " 'minister': 271,\n",
       " 'uk': 272,\n",
       " 'margaretthatcher': 273,\n",
       " 'fastest': 274,\n",
       " 'animal': 275,\n",
       " 'cheetah': 276,\n",
       " 'periodic': 277,\n",
       " 'table': 278,\n",
       " 'spain': 279,\n",
       " 'madrid': 280,\n",
       " 'closest': 281,\n",
       " 'sun': 282,\n",
       " 'father': 283,\n",
       " 'computers': 284,\n",
       " 'charlesbabbage': 285,\n",
       " 'mexico': 286,\n",
       " 'mexicocity': 287,\n",
       " 'colors': 288,\n",
       " 'rainbow': 289,\n",
       " 'musical': 290,\n",
       " 'instrument': 291,\n",
       " 'black': 292,\n",
       " 'white': 293,\n",
       " 'keys': 294,\n",
       " 'piano': 295,\n",
       " 'americas': 296,\n",
       " '1492': 297,\n",
       " 'christophercolumbus': 298,\n",
       " 'disney': 299,\n",
       " 'character': 300,\n",
       " 'long': 301,\n",
       " 'nose': 302,\n",
       " 'grows': 303,\n",
       " 'it': 304,\n",
       " 'when': 305,\n",
       " 'lying': 306,\n",
       " 'pinocchio': 307,\n",
       " 'directed': 308,\n",
       " 'movie': 309,\n",
       " 'titanic': 310,\n",
       " 'jamescameron': 311,\n",
       " 'superhero': 312,\n",
       " 'also': 313,\n",
       " 'dark': 314,\n",
       " 'knight': 315,\n",
       " 'batman': 316,\n",
       " 'brasilia': 317,\n",
       " 'fruit': 318,\n",
       " 'king': 319,\n",
       " 'fruits': 320,\n",
       " 'mango': 321,\n",
       " 'eiffel': 322,\n",
       " 'tower': 323}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00292050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(text):\n",
    "    indexed_text = []\n",
    "    for token in tokenize(text):\n",
    "        if token in vocab:\n",
    "            indexed_text.append(vocab[token])\n",
    "        else :\n",
    "            indexed_text.append(vocab[\"<UNK>\"])\n",
    "    return indexed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad4a9af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 24, 25, 5, 26, 19, 27]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indices(\"What is the boiling point of water in Celsius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb9c58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return torch.tensor(text_to_indices(self.df.iloc[index][\"question\"])) , torch.tensor(text_to_indices(self.df.iloc[index][\"answer\"])[0])\n",
    "\n",
    "train_dataset = CustomDataset(df)\n",
    "train_loder = DataLoader(dataset=train_dataset,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "896af8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  2,  3,  4,  5, 53]), tensor(54))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6c2283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([170])\n",
      "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([114])\n",
      "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([121])\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) tensor([7])\n",
      "tensor([[ 10, 308,   3, 309, 310]]) tensor([311])\n",
      "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([65])\n",
      "tensor([[10,  2,  3, 66,  5, 67]]) tensor([68])\n",
      "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([260])\n",
      "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([166])\n",
      "tensor([[ 10,  29, 130, 131]]) tensor([132])\n",
      "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([298])\n",
      "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([36])\n",
      "tensor([[ 10,  96,   3, 104, 239]]) tensor([240])\n",
      "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([128])\n",
      "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([162])\n",
      "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([49])\n",
      "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([307])\n",
      "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([295])\n",
      "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([95])\n",
      "tensor([[ 10,  75, 208]]) tensor([209])\n",
      "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([184])\n",
      "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([113])\n",
      "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([149])\n",
      "tensor([[ 10,  75, 111]]) tensor([112])\n",
      "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([207])\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([116])\n",
      "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([220])\n",
      "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([134])\n",
      "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([124])\n",
      "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([99])\n",
      "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([28])\n",
      "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([36])\n",
      "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([225])\n",
      "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([233])\n",
      "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([106])\n",
      "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([85])\n",
      "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([268])\n",
      "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([136])\n",
      "tensor([[10, 55,  3, 56,  5, 57]]) tensor([58])\n",
      "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([246])\n",
      "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([100])\n",
      "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([91])\n",
      "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([52])\n",
      "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([205])\n",
      "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([205])\n",
      "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([23])\n",
      "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([316])\n",
      "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([249])\n",
      "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([110])\n",
      "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([194])\n",
      "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([61])\n",
      "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([179])\n",
      "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([199])\n",
      "tensor([[10, 96,  3, 97]]) tensor([98])\n",
      "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([273])\n",
      "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([317])\n",
      "tensor([[10, 29,  3, 30, 31]]) tensor([32])\n",
      "tensor([[10, 11, 12, 13, 14, 15]]) tensor([16])\n",
      "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([280])\n",
      "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([6])\n",
      "tensor([[10, 75, 76]]) tensor([77])\n",
      "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([238])\n",
      "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([41])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([36])\n",
      "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([215])\n",
      "tensor([[1, 2, 3, 4, 5, 8]]) tensor([9])\n",
      "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([254])\n",
      "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([321])\n",
      "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([72])\n",
      "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([244])\n",
      "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([121])\n",
      "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([156])\n",
      "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([54])\n",
      "tensor([[ 10,  11, 189, 158, 190]]) tensor([191])\n",
      "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([53])\n",
      "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([287])\n",
      "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([155])\n",
      "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([145])\n",
      "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([154])\n",
      "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([74])\n",
      "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([85])\n",
      "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([276])\n",
      "tensor([[ 10,  11, 157, 158, 159]]) tensor([160])\n",
      "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([188])\n",
      "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([285])\n",
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([185])\n",
      "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([173])\n",
      "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([131])\n",
      "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([259])\n",
      "tensor([[ 42, 101,   2,   3,  17]]) tensor([102])\n"
     ]
    }
   ],
   "source": [
    "for question , answer in train_loder:\n",
    "    print(question , answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4aa8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleRNN(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim=50)\n",
    "        self.rnn = nn.RNN(50,64,batch_first=True)\n",
    "        self.out = nn.Linear(64,vocab_size)\n",
    "    def forward(self, X):\n",
    "        embeded_question = self.embedding(X)\n",
    "        hidden , final = self.rnn(embeded_question)\n",
    "        output = self.out(final).squeeze(0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "687b76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac1a419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MySimpleRNN(len(vocab))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c8de1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  --->  loss : 518.0644617080688\n",
      "Epoch:  2  --->  loss : 447.4632263183594\n",
      "Epoch:  3  --->  loss : 367.93790435791016\n",
      "Epoch:  4  --->  loss : 308.1329391002655\n",
      "Epoch:  5  --->  loss : 256.96036410331726\n",
      "Epoch:  6  --->  loss : 209.11036431789398\n",
      "Epoch:  7  --->  loss : 165.1399273276329\n",
      "Epoch:  8  --->  loss : 128.4382963180542\n",
      "Epoch:  9  --->  loss : 97.67245382070541\n",
      "Epoch:  10  --->  loss : 74.2771055996418\n",
      "Epoch:  11  --->  loss : 57.02625694870949\n",
      "Epoch:  12  --->  loss : 44.40392832458019\n",
      "Epoch:  13  --->  loss : 35.346337616443634\n",
      "Epoch:  14  --->  loss : 28.588118866086006\n",
      "Epoch:  15  --->  loss : 23.47801499068737\n",
      "Epoch:  16  --->  loss : 19.758972741663456\n",
      "Epoch:  17  --->  loss : 16.736809514462948\n",
      "Epoch:  18  --->  loss : 14.169082626700401\n",
      "Epoch:  19  --->  loss : 12.296500772237778\n",
      "Epoch:  20  --->  loss : 10.691181667149067\n",
      "Epoch:  21  --->  loss : 9.3989700935781\n",
      "Epoch:  22  --->  loss : 8.250853691250086\n",
      "Epoch:  23  --->  loss : 7.3054195791482925\n",
      "Epoch:  24  --->  loss : 6.520881816744804\n",
      "Epoch:  25  --->  loss : 5.8309958800673485\n",
      "Epoch:  26  --->  loss : 5.277444014325738\n",
      "Epoch:  27  --->  loss : 4.771894281730056\n",
      "Epoch:  28  --->  loss : 4.350445222109556\n",
      "Epoch:  29  --->  loss : 3.9734073895961046\n",
      "Epoch:  30  --->  loss : 3.6299668569117785\n",
      "Epoch:  31  --->  loss : 3.342887867242098\n",
      "Epoch:  32  --->  loss : 3.0819129534065723\n",
      "Epoch:  33  --->  loss : 2.843565819784999\n",
      "Epoch:  34  --->  loss : 2.634712727740407\n",
      "Epoch:  35  --->  loss : 2.440072391182184\n",
      "Epoch:  36  --->  loss : 2.270017179660499\n",
      "Epoch:  37  --->  loss : 2.1140218852087855\n",
      "Epoch:  38  --->  loss : 1.9686489570885897\n",
      "Epoch:  39  --->  loss : 1.838708815164864\n",
      "Epoch:  40  --->  loss : 1.7189707541838288\n",
      "Epoch:  41  --->  loss : 1.6095297215506434\n",
      "Epoch:  42  --->  loss : 1.5080953864380717\n",
      "Epoch:  43  --->  loss : 1.4163397578522563\n",
      "Epoch:  44  --->  loss : 1.3309610541909933\n",
      "Epoch:  45  --->  loss : 1.2503279992379248\n",
      "Epoch:  46  --->  loss : 1.1772545580752194\n",
      "Epoch:  47  --->  loss : 1.1084259580820799\n",
      "Epoch:  48  --->  loss : 1.0443049361929297\n",
      "Epoch:  49  --->  loss : 0.9858108274638653\n",
      "Epoch:  50  --->  loss : 0.9299011966213584\n",
      "Epoch:  51  --->  loss : 0.8772291969507933\n",
      "Epoch:  52  --->  loss : 0.8308866745792329\n",
      "Epoch:  53  --->  loss : 0.7849452346563339\n",
      "Epoch:  54  --->  loss : 0.7421644357964396\n",
      "Epoch:  55  --->  loss : 0.7030298253521323\n",
      "Epoch:  56  --->  loss : 0.6654088171198964\n",
      "Epoch:  57  --->  loss : 0.6309622027911246\n",
      "Epoch:  58  --->  loss : 0.5971821963321418\n",
      "Epoch:  59  --->  loss : 0.5669518071226776\n",
      "Epoch:  60  --->  loss : 0.5374917106237262\n",
      "Epoch:  61  --->  loss : 0.5098602664656937\n",
      "Epoch:  62  --->  loss : 0.484673973871395\n",
      "Epoch:  63  --->  loss : 0.45977845741435885\n",
      "Epoch:  64  --->  loss : 0.43673953507095575\n",
      "Epoch:  65  --->  loss : 0.4150962003041059\n",
      "Epoch:  66  --->  loss : 0.3939890239853412\n",
      "Epoch:  67  --->  loss : 0.3752788172569126\n",
      "Epoch:  68  --->  loss : 0.3565080986591056\n",
      "Epoch:  69  --->  loss : 0.3389847504440695\n",
      "Epoch:  70  --->  loss : 0.322567633818835\n",
      "Epoch:  71  --->  loss : 0.3070552672725171\n",
      "Epoch:  72  --->  loss : 0.29225202510133386\n",
      "Epoch:  73  --->  loss : 0.2782325966982171\n",
      "Epoch:  74  --->  loss : 0.26527004258241504\n",
      "Epoch:  75  --->  loss : 0.25231625919695944\n",
      "Epoch:  76  --->  loss : 0.2402567124227062\n",
      "Epoch:  77  --->  loss : 0.22888564760796726\n",
      "Epoch:  78  --->  loss : 0.21795698255300522\n",
      "Epoch:  79  --->  loss : 0.20799025159794837\n",
      "Epoch:  80  --->  loss : 0.19812884624116123\n",
      "Epoch:  81  --->  loss : 0.18879674305208027\n",
      "Epoch:  82  --->  loss : 0.18022789171664044\n",
      "Epoch:  83  --->  loss : 0.1716353032970801\n",
      "Epoch:  84  --->  loss : 0.16373689961619675\n",
      "Epoch:  85  --->  loss : 0.15614002029178664\n",
      "Epoch:  86  --->  loss : 0.14876873779576272\n",
      "Epoch:  87  --->  loss : 0.14190722483908758\n",
      "Epoch:  88  --->  loss : 0.1354406672762707\n",
      "Epoch:  89  --->  loss : 0.12923259905073792\n",
      "Epoch:  90  --->  loss : 0.12332843767944723\n",
      "Epoch:  91  --->  loss : 0.11758550850208849\n",
      "Epoch:  92  --->  loss : 0.11224876146297902\n",
      "Epoch:  93  --->  loss : 0.1071169106871821\n",
      "Epoch:  94  --->  loss : 0.10226952593075112\n",
      "Epoch:  95  --->  loss : 0.09768233849899843\n",
      "Epoch:  96  --->  loss : 0.09320840399595909\n",
      "Epoch:  97  --->  loss : 0.0889602672541514\n",
      "Epoch:  98  --->  loss : 0.08496615718468092\n",
      "Epoch:  99  --->  loss : 0.0811263132491149\n",
      "Epoch:  100  --->  loss : 0.0774716301530134\n"
     ]
    }
   ],
   "source": [
    "for epoche in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_features , batch_labels in train_loder:\n",
    "        \n",
    "        y_pred = model(batch_features)\n",
    "        \n",
    "        loss = loss_function(y_pred,batch_labels.long())\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    print('Epoch: ', epoche+1,\" ---> \",\"loss :\",epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caaa0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, question, threshold=0.5):\n",
    "\n",
    "  # convert question to numbers\n",
    "  numerical_question = text_to_indices(question)\n",
    "\n",
    "  # tensor\n",
    "  question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
    "\n",
    "  # send to model\n",
    "  output = model(question_tensor)\n",
    "\n",
    "  # convert logits to probs\n",
    "  probs = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "  # find index of max prob\n",
    "  value, index = torch.max(probs, dim=1)\n",
    "\n",
    "  if value < threshold:\n",
    "    print(\"I don't know\")\n",
    "  else:\n",
    "    print(list(vocab.keys())[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d543231f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupiter\n"
     ]
    }
   ],
   "source": [
    "predict(model, \"largest planet our solar system?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7b69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
